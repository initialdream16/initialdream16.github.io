---
layout:     post
title:      hue
subtitle:   大数据交互平台
date:       2021-12-31
author:     果然
header-img: img/the-first.png
catalog:   true
tags:
    - hue
---

记录内网使用 hue 流程。   
因为需要对二进制数据进行解析，二进制数据的字节起始情况保存在mysql中，在查看之前，我们首先创建一个只读用户，防止误删等操作造成数据库数据丢失等失误。  
# MySQL
1. 查看数据库中所有用户
```
use mysql;
select host,user from user;
```
2. 创建一个只读用户
```
grant select on *.* to 'username'@'%' identified by "password";
flush privileges;
```
3. 数据库当前用户
```
select user();
select current_user;
```
4. 使用新用户登录报错(using passwrd:Yes)
删除用户名为空的账户  
```
delete from user where user='';
flush privileges;
```
5. 查看用户权限
```
show grant for 'username'@'host';
```
# 进制、字节间转换(Python 函数)
## 1. 十进制转换为二进制、八进制、十六进制

bin():返回一个整数 int或者长整数 long int 的二进制表示
oct():将一个整数转换成8进制字符串
hex():将制定数字转换为16进制值

1 byte = 8bit(二进制值)
int(a, 2)/int(a,8)/int(a,16)分别将2/8/16进制数转换为整数

## 2. byte 转换为 int 类型函数
int.from_bytes 
```
s1 = b'\xf1\xff'
print(int.from_bytes(s1,byteorder='big',signed=False))
print(int.from_bytes(s1,byteorder='little',signed=True))
```
参数解释：
bytes：要转换的十六进制
byteorder：big 代表正常顺序。little代表反序
signed：是否要区分二进制的正负数。即是否要对原二进制数进行源码反码补码操作

## 3. int  转 byte 
```
int(1234).to_bytes(length=2,byteorder='big',signed=true)
```
参数解释：
int：代表要被转换的数字
length：代表要转换几个字节
byteorder：big 代表高位在前，反之低位在前

## 4. 原码 反码  补码 
原码：为了表示负数(改变开头数字，1表示负、0表示正)
      (1)2=0001        (-1)2=1001
反码：为了处理负数(将负数的原码除了开头位全部由0变1或者由1变0)
补码：正数的补码等于它本身，负数的补码=其反码 + 1

# [python 操作 hdfs](https://blog.csdn.net/oTengYue/article/details/88193115)

将本地解析好数据上传至大数据中心资源的 HDFS 上。这里，尝试使用 pyhdfs(使用 Python 连接 HDFS 进行操作)。  
首先需要在 hdfs-site.xml 中配置：
```
<property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
</property>
```

```
import pyhdfs
client = pyhdfs.HdfsClient(hosts="xxx:50070",user_name="hadoop")
client.get_home_directory()         // 用户根目录
client.get_active_namenode()        // 可用的 namenode 节点
client.listdir("/user")             // 指定目录下的所有文件
client.open("/student.txt").read()         // 打开文件并读取内容
client.copy_from_local("local_path","hdfs_path")    // 从本地上传文件至集群
client.append("hdfs_path","content")         // 追加数据
client.mkdirs("/user/hadoop/data")           // 创建新目录
client.list_status("xxx")                    // 查看单个文件状态
``` 
存在问题(https://blog.csdn.net/shen_ness/article/details/82786261)：requests.exceptions.ConnectionError: HTTPConnectionPool
在 windows 下的 hosts 文件中添加hdfs的ip和hostname。

最终，因为远程大数据中心对 python 安装包不足等原因，使用 sqoop 对历史数据进行操作。  
hue(sqoop)
使用sqoop 将MySQL数据传至hdfs(https://stackoverflow.com/questions/51463446/cant-run-shell-in-oozie-error-2-no-such-file-or-directory)  
将CRLF line -->  LF (\n).  


# hive 三种连接方法
## CLI 连接
```
cd /usr/hive/bin
./hive
```
上述的 hive 命令相当于在启动的时候执行: hive --service cli。
## HiveServer2/beeline
首先需要启动 webhdfs(dfs.webhdfs.enabled=true)

设置 hadoop 的代理用户
hadoop.proxyuser.hadoop.hosts/hadoop.proxyuser.hadoop.groups (*)
表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群/表示代理用户的组所属。  
第一步：先启动 hiveserver2服务
```
hiveserver2
或启动为后台
nohup hiveserver2 1>/home/hadoop/hiveserver.log 2>/home/hadoop/hiveserver.err &
nohup hiveserver2 1>/dev/null 2>/dev/null &
nohup hiveserver2 > /dev/null 2 >&1 &
```
第二步：启动 beeline 客户端去连接以上服务端
```
beeline -u jdbc:hive2//localhost:10000 -n hadoop
```
-u：指定元数据库的链接信息
-n: 指定用户名和密码  
## hive web  
hive 2.0 以后才支持 web UI：  
```
./bin/hive-service hwi
```  
# 一些问题记录
1. [从Sqoop导入MySQL导入TINYINT(1)类型数据到hive（tinyint），数据为null](https://blog.csdn.net/qq_43688472/article/details/117998545)
sqoopimport  导入 tinyint类型数据至hive，数据为 null。
添加：jdbc:mysql://14.21.xx.21:51x3x/${database}?zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false
2. [Hive中的Timestamp类型日期与Impala中显示不一致分析](https://cloud.tencent.com/developer/article/1077819)
