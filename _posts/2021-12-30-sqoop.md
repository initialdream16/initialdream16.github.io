---
layout:     post
title:      sqoop
subtitle:   用于在hadoop(hive)与传统数据库间进行数据传递
date:       2021-12-30
author:     果然
header-img: img/timg.jpg
catalog: true
tags:
    - 大数据组件技术选型
---

sqoop 是一款开源的工具，主要用于在 hadoop(hive)与传统的数据库(mysql/postgresql...)间进行数据的传递，可以将一个关系型数据库中的数据导进到 hadoop 的 HDFS(hive/hbase...) 中，也可以将 HDFS 的数据导进到关系型数据库中。  

Apache Sqoop 是可以在 hadoop和关系型数据库之间转移大量数据的一款工具。  

[centos7 下安装 sqoop](https://blog.csdn.net/qq_21153619/article/details/81866722)  

## 一些遇到的问题
-1. **“Retrying connect to server: 0.0.0.0/0.0.0.0:8032” 的错误**  

mapred-site.xml 重命名为 mapred-site.xml.template  
这是因为：YARN 主要是为集群提供更好地资源管理与任务调度，在单机上体现不出价值，不开启YARN时，需将上述配置文件重命名。  

-2. **Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf**  

将 hive lib目录下的 hive 相关包拷贝至 sqoop 的 lib 下即可。  
这是因为 sqoop 安装目录中没有 hive 相关包。  
```
cp /usr/local/hive/lib/hive* /usr/local/sqoop/lib/
```  

-3. **java.lang.ClassNotFoundException:org.apache.logging.log4j.spi.LoggerContextFactory**  
## sqoop 的数据导入与导出  
查看数据库的名称：*sqoop list-databases -D mapreduce.job.queuename=root.dev --connect jdbc:mysql://ip:3306/ --username xxx --password xxx*  
列出数据库中表名：*sqoop list-tables -D mapreduce.job.queuename=root.dev --connect jdbc:mysql://ip:3306/database_name --username xxx --password xxx*  
导入数据至hdfs(hive)：  
```
sqoop import -D mapreduce.job.queuename=root.dev \
--connect jdbc:mysql://ip:3306/database_name \     # 指定JDBC的URL                      
--table tablename \                                # 读取表中数据
--username xxx --password xxx \                    
--target-dir /path \                               # HDFS中导入表的存放目录
--fields-terminated-by '\t' \                      # 设定导入数据后每个字段的分隔符  默认;分割
--lines-terminated-by '\n' \                       # 设定导入数据后每行的分隔符
--m 1 \                                            # 并发的map数量为1，不设置默认启动4个map task，则需要指定一个列来作为划分map task 任务的依据
--where query \                                    # 导入查询出来的内容，表的子集
--incremental append \                             # 增量导入
--check-column:column_id \                         # 指定增量导入时的参考列
--last-value:num \                                 # 上一次导入 column_id 的最后一个值
--null-string ''                                   # 导入字段为空时，用指定的字符进行替换
```  
导出数据至数据库（MySQL）:  
```
sqoop export -D mapreduce.job.queuename=root.dev \
--connect jdbc:mysql:ip:3306/database_name \
--username xxx --password xxx \
--table tablename \
--export-dir /path \                                # hdfs上的数据文件
--fields-terminated-by '\t' \
--lines-terminated-by '\n' \
--m 1 \
--incremental append \
--check-column:column_id \
--last-value:num \
--null-string ''
```  
在导入之前，还需要创建hive临时表和MySQL表。  





